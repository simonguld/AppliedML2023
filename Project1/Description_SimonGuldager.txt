[Intro here]

1: Classification_SimonGuldager_XGBoost.txt:
  Algorithm: XGBoost XGBClassifier
  Key HP values: learning_rate=0.213, n_estimators=86, max_depth=5 
  HP optimisation: HP was tuned using optuna for 30 iterations (Each one 5-fold cross validated)
  Size of model: 297 kB
  Pre-processing: Scaled the input features using StandardScaler. 
  Loss function and value on validation set: 0.1539 (binary cross entropy) with 5-fold cross validation
  Own evaluation: Very good model [AUC = 0.976, acc = 0.940], and 15 best variables (from build-in ranking) 
                  was clear.

2. Classification_SimonGuldager_LGB_GBDT.txt:
  Algorithm: Light GBM's classifier with GDBT booster
  Key HP values: {'importance_type': 'gain', 'n_estimators': 249, 'learning_rate': 0.108, 'max_depth': 5, 
                  'num_leaves': 162, 'subsample': 0.996}
  HP optimisation:  HP was tuned using optuna for 30 iterations (Each one 5-fold cross validated)
  Size of model: 813 kB
  Pre-processing: Scaled the input features using StandardScaler. 
  Loss function and value on validation set: 0.1519 (binary cross entropy) with 5-fold cross validation
  Own evaluation: Great model [AUC = 0.976, acc = 0.941], sligtly outperformining XGBoost. 
                  I initally wanted to classify using LGBM's Random Forest, but it performed poorly with
                  BCE > 0.20 (as did Sklearn's).
                  I tried using the algorithms feature ranking attribute to select the best 15 features,
                  but performance suffered somewhat. Using XGBoost's choice for best features, however,
                  the algorithm suffered no loss in performance as compared to using all features.
                  I have thus chosen XGBoost's top 15 features.

3: Classification_SimonGuldager_SKLEARN_MLP.txt:
  Algorithm: SKLearns MLPClassifier
  Key HP values: hidden_layer_sizes=(15, 64, 128, 256, 256, 128, 64), activation='relu', solver='adam',
                 batch_size=64, learning_rate= 0.00173, max_iter=25, \
  HP optimisation: First, the complexity was increased until no further improvements were made. Then, the
			 learning rate as well as the batch size were optimized using CVRandomizedSearch. The no.
			 of iterations were then chosen as that necessary to minimize the validation loss.
  Size of model: 3.43 MB
  Pre-processing: Scaled the input features using StandardScaler. 
  Loss function and value on validation set: 0.1743 (Binary cross entropy, 5-fold cross validation)
  Own evaluation: Descent model, slow but easy to train. It outperforms random forests (Both LGBM and SKLearn),
			            which are stuck at around 0.20 (Binary cross entropy), but is no match for XGBoost. 
                  The best 15 variables were chosen as those selected by XGBoost, and as it turned out,
		              the neural network suffered no loss in performance when training on these as opposed to all features. 
          			  Since tree feature ranking is orders of magnitude faster than that of NNs, I was quite pleased.

4. Regression_SimonGuldager_LGB_GBDT.txt:
  Algorithm: Light GBM's regressor with GBDT boosting
  Key HP values: {'importance_type': 'gain', 'n_estimators': 336, 'learning_rate': 0.0641, 'max_depth': 12, \
                  'num_leaves': 51, 'min_child_samples': 4, 'subsample': 0.894}
  HP optimisation:  HP was tuned using optuna for 50 iterations (Each one 5-fold cross validated)
  Size of model: 1.57 MB
  Pre-processing: Scaled the input features using StandardScaler. No scaling of output features
  Loss function and value on validation set: 0.0658 (MAE) 
  Own evaluation: Great model, improving slightly on perfomance and greatly on speed as compared to XGBoost
                  and Tensorflow. The best 20 best variables (from build-in ranking) was clear.

5: Regression_SimonGuldager_Tensorflow_NN.txt:
  Algorithm: keras.sequential and KerasRegressor using a fully connected neural network
  Key HP values: Ninput = 20, Nhidden1=64, Nhidden2=128, Nhidden3=256, Nhidden4 = 128, Nhidden5 = 64,
		     LearningRate=Variable (from 0.013 to 0.004), batch size = 128, epochs = 70
  HP optimisation: The inital learning rate (where to start the learning schedule) as well as the batch size
                   were optimized using a simple grid search. This was done after the following steps
                  Step 1: Find a sufficiently complex architechture to capture data, as evaluated by increasing 
				                  complexity until the validation loss started increasing. 
                  Step 2: Use batch normalization to allow for bigger learning rates. Use a piecewise constant 
			                    decreasing learning schedule.
                  Step 3: Find optimal values of initial learning rate and batch size by gridsearch with
				                  5-fold crossvalidation 
                  Step 4: Train model until validation loss is minimized            
  Parameters in model: 84,325 (340 kB)
  Pre-processing: Scaled the input features using StandardScaler. No scaling of output features
  Loss function and value on validation set: 0.0664 (MAE)
  Own evaluation: Great model, matching the LGB GDBT model in performance. The best 20 variables were chosen as those
 			            selected by LGBM, and as it turned out, the neural network suffered no loss in performance when 
			            training on these as opposed to all features. Since tree feature ranking is orders of magnitude faster
			            than that of NNs, I was quite pleased.

6. Regression_SimonGuldager_SKLearn_RandomForest.txt:
  Algorithm: SKLearn's RandomForestRegressor
  Key HP values: n_estimators=356, max_depth=22, min_samples_leaf=15, max_features='sqrt', max_samples=0.826
  HP optimisation:  HP was tuned using CVRandomized search for 30 iterations (with 5-fold cross validation)
  Size of model: 128 MB
  Pre-processing: Scaled the input features using StandardScaler. No scaling of output features
  Loss function and value on validation set: 0.0728 (MAE) 
  Own evaluation: Solid model, though slow and outperformed by LGBM. It does a much better job at regression than
                  at classification for our data. The size of the model is, admittedly, somewhat ridiculous,
                  and one could reduce the model size considerably by letting max_depth < 12 while only
                  hurting performance slightly.
                  The best 20 best variables have simply been chosen as those selected by the LGBM algorithm,
                  as the Random Forest suffered no loss in performance when using just those 20 features as
                  compared with using all features.


7: Clustering_SimonGuldager_SKLearn_MiniBatchKMeans.txt:
  Algorithm: Sklearn MiniBatchKMeans
  Key HPs: n_clusters = 18, max_no_improvement = 25, n_init = 5
  HP optimisation: Tested various values of n_clusters. 
  Model size: 524 kB
  Pre-processing: Scaled the input features using StandardScaler
  Loss function and value on training (!) set: 93585.37 (Inertia = sum of square distances of samples to their 
                                               cluster center). It does not provide a value when predicting.
                                               Also [on validation set]: 0.32 (Silhouette value, c.f. 
                                               https://en.wikipedia.org/wiki/Silhouette_(clustering))
  Own evaluation: The inertia has a slight 'elbow' around 18, i.e the slope of the graph flattens somewhat for
                   n > 18, and so I have used the rather vague elbow princple. This was combined with the
                  silhouette method, seeking to find the maximum of the silhouette value (see wiki.).
                  Since the 2 largest silhouette values did not correspond to an 'elbow' point, I chose
                  the third largest silhouette value at n = 18.  
                  As a cross check, I estimated the clustering's ability to make clusters fitting either into
                  the electron or non-electron category. It was able to separate correctly in 90% of cases for 
                  both training and validation data (weighted separation of all clusters).
                  I tried to find the best 5 features using various methods, and ended up using the top 5
                  features selected by the XGBoost classifier. With these 5, it was still able to classify with a 
                  92% accuracy.

8: Clustering_SimonGuldager_SKLearn_Birch.txt:
  Algorithm: Sklearn Birch
  Key HPs: n_clusters = 43, threshold = 0.4, branching_factor = 250
  HP optimisation: Performed a 'homemade' grid search of the 3 hyperparameters to optimize the silhouette score
  Model size: 2.77 MB
  Pre-processing: Scaled the input features using StandardScaler
  Loss function and value on validation set: 0.560 (Silhouette value, c.f. 
                                               https://en.wikipedia.org/wiki/Silhouette_(clustering))
  Own evaluation: OK model, does well in terms of the Silhouette score. It is not clear if
                  there is structure to justify 43 clusters - this has been chosen merely so as to
                  maximize the silhoutte score, which is a measure of separability between clusters.
                  It outperforms KMinibatch in terms of this score. 
                  As a cross check, I estimated the clustering's ability to make clusters fitting either into
                  the electron or non-electron category. It was able to separate correctly in 88% of cases for 
                  both training and validation data (weighted separation of all clusters).
                  I tried to find the best 5 features using various methods, and ended up using the top 5
                  features selected by the XGBoost classifier. With these 5, it was still able to classify with a 
                  92% accuracy.

9: Clustering_SimonGuldager_SKLearn_BisectingKMeans.txt:
  Algorithm: Sklearn Bisecting KMeans
  Key HPs: n_clusters = 4, bisecting_strategy = 'largest_cluster', init = 'k-means++'
  HP optimisation: Calculated intertia and silhouette score for all choices of n_cluster in [3,50]
  Model size: 544 kB
  Pre-processing: Scaled the input features using StandardScaler
  Loss function and value on training (!) set: 337087 (Inertia = sum of square distances of samples to their 
                                               cluster center). It does not provide a value when predicting.
                                               Also [on validation set]: 0.58 (Silhouette value, c.f. 
                                               https://en.wikipedia.org/wiki/Silhouette_(clustering))
  Own evaluation: Descent model. I would have like to do a DBScan or an agglomerative algorithm,
			but my RAM was maxed out. I then moved on the ERDA, but the kernels kept dying when trying
			to fit these algorithms to the entire dataset. I therefore went with another KMeans
			based algorithm.
			The inertia has a very clear 'elbow' at n_cluster = 4, and since the silhouette score
			is also maximized at this no. of clusters, n_cluster = 4 seems like a natural choice.
                  As a cross check, I estimated the clustering's ability to make clusters fitting either into
                  the electron or non-electron category. It was able to separate correctly in 88% of cases for 
                  both training and validation data (weighted separation of all clusters).
                  I tried to find the best 5 features using various methods, and ended up using the top 5
                  features selected by the XGBoost classifier. With these 5, it was still able to classify with a 
                  92% accuracy.